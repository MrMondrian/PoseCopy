#!/usr/bin/env python3

# -*- coding: utf-8 -*-
import rospy
import cv2
import math
import numpy as np
import mediapipe as mp
from mediapipe.python.solutions.pose import PoseLandmark

"""mediapipe_pose.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uCuA6We9T5r0WljspEHWPHXCT_2bMKUy

Usage example of MediaPipe Pose Solution API in Python (see also http://solutions.mediapipe.dev/pose).
"""


"""Upload any image that that has a person. We take two example images from the web: https://unsplash.com/photos/v4zceVZ5HK8 and https://unsplash.com/photos/e_rhazQLaSs.

"""


mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils 
mp_drawing_styles = mp.solutions.drawing_styles


class PoseCapture:


  def __init__(self,video=None):
    self.DESIRED_HEIGHT = 480
    self.DESIRED_WIDTH = 480
    if video == None:
      self.cap = cv2.VideoCapture(0)
    else:
      self.cap = cv2.VideoCapture(video)



  def resize_and_show(self, image):
    h, w = image.shape[:2]
    if h < w:
      img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
    else:
      img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
    cv2.imshow("",img)

  def angle(self, v1,v2,z_scale):
    v1[-1] /= z_scale
    v2[-1] /= z_scale
    dot = sum([v1[i] * v2[i] for i in range(len(v1))])
    norm1 = math.dist([0]*len(v1),v1)
    norm2 = math.dist([0]*len(v1),v2)
    #print(dot/(norm1*norm2))
    return math.acos(dot/(norm1*norm2))


  def landmarks_to_dict(self, landmarks):
    out = []
    # print("##############################")
    # print(landmarks.landmark)
    for (i,lm) in enumerate(landmarks.landmark):
      out.append(lm)
    return out

  def dict_to_list(self, d):
    return [d.x,d.y,d.z]

  def elbow_angle(self, results):
    landmarks = results.pose_world_landmarks
    lm_dict = self.landmarks_to_dict(landmarks)
    wrist = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_WRIST])
    elbow = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_ELBOW])
    shoulder = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_SHOULDER])

    v1 = [shoulder[i] - elbow[i] for i in range(3)]
    v2 = [wrist[i] - elbow[i] for i in range(3)]
    return self.angle(v1,v2,30)

  def shoulder_angle(self, results):
    landmarks = results.pose_world_landmarks
    lm_dict = self.landmarks_to_dict(landmarks)
    hip = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_HIP])
    elbow = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_ELBOW])
    shoulder = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_SHOULDER])

    v1 = [elbow[i] - shoulder[i] for i in range(3)]
    v2 = [hip[i] - shoulder[i] for i in range(3)]
    return self.angle(v1,v2,15)

  def shoulder_rotation(self, results):
    landmarks = results.pose_world_landmarks
    lm_dict = self.landmarks_to_dict(landmarks)
    elbow = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_ELBOW])
    shoulder = self.dict_to_list(lm_dict[PoseLandmark.RIGHT_SHOULDER])

    v1 = [elbow[0] - shoulder[0], elbow[2] - shoulder[2]]
    v2 = [1,0]

    return self.angle(v1,v2,7)




  def angles(self):
    with mp_pose.Pose(
      static_image_mode=True,
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        model_complexity=2) as pose:
      while self.cap.isOpened():
        success, image = self.cap.read()
        if not success:
          print("Ignoring empty camera frame.")
          # If loading a video, use 'break' instead of 'continue'.
          continue

        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = pose.process(image)
        yield [self.shoulder_rotation(results),self.shoulder_angle(results),self.elbow_angle(results),0,0,0]
        #print(elbow_angle(results))
        #print(shoulder_angle(results))
        # if(results.pose_world_landmarks and results.pose_world_landmarks.landmark):
        #   print("eblow: {}, shoulder: {}, roations: {} ".format(elbow_angle(results),shoulder_angle(results),shoulder_rotation(results)))
        #elbow_angle(results)
        #print(results.pose_landmarks)
        # print(mp_pose.POSE_CONNECTIONS)
        # Draw the pose annotation on the image.
        # image.flags.writeable = True
        #print(elbow_angle(results))
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        mp_drawing.draw_landmarks(
            image,
            results.pose_landmarks,
            mp_pose.POSE_CONNECTIONS,
            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
        cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))
        if cv2.waitKey(5) & 0xFF == 27:
          break
        # mp_drawing.plot_landmarks(
        #     results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)
    self.cap.release()

  def angle_from_image(self,path):
        with mp_pose.Pose(
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        model_complexity=2) as pose:
            image = cv2.imread(path, cv2.IMREAD_COLOR)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            results = pose.process(image)
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
            mp_drawing.draw_landmarks(
              image,
              results.pose_landmarks,
              mp_pose.POSE_CONNECTIONS,
              landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
            cv2.imwrite('annotated_image.png', image)
            return [self.shoulder_rotation(results),abs(3.14 -self.shoulder_angle(results)),abs(3.14 - self.elbow_angle(results)),0,0,0]

if __name__ == '__main__':
  thing = PoseCapture("example.mp4")
  #while True:
  image_angles = thing.angle_from_image('/home/anthony/comp400/sim/kinova-arm/catkin_ws/src/base_controller/scripts/example3.jpg')
  # for angles in thing.angles():
  #   print(angles)


'''
# Run MediaPipe Pose and draw pose landmarks.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    # Convert the BGR image to RGB and process it with MediaPipe Pose.
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    print(elbow_angle(results))
    # Print nose landmark.
    image_hight, image_width, _ = image.shape
    if not results.pose_landmarks:
      continue
    print(
      f'Nose coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_hight})'
    )

    # Draw pose landmarks.
    print(f'Pose landmarks of {name}:')
    annotated_image = image.copy()
    mp_drawing.draw_landmarks(
        annotated_image,
        results.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
    resize_and_show(annotated_image)

# Run MediaPipe Pose and plot 3d pose world landmarks.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Print the real-world 3D coordinates of nose in meters with the origin at
    # the center between hips.
    print('Nose world landmark:'),
    print(results.pose_world_landmarks.landmark[mp_pose.PoseLandmark.NOSE])
    
    # Plot pose world landmarks.
    mp_drawing.plot_landmarks(
        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)

# Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, 
    model_complexity=2, enable_segmentation=True) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw pose segmentation.
    print(f'Pose segmentation of {name}:')
    annotated_image = image.copy()
    red_img = np.zeros_like(annotated_image, dtype=np.uint8)
    red_img[:, :] = (255,255,255)
    segm_2class = 0.2 + 0.8 * results.segmentation_mask
    segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)
    annotated_image = annotated_image * segm_2class + red_img * (1 - segm_2class)
    resize_and_show(annotated_image)
  '''
