#!/usr/bin/env python3

# -*- coding: utf-8 -*-
import rospy
import cv2
import math
import numpy as np
import mediapipe as mp
from mediapipe.python.solutions.pose import PoseLandmark

"""mediapipe_pose.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uCuA6We9T5r0WljspEHWPHXCT_2bMKUy

Usage example of MediaPipe Pose Solution API in Python (see also http://solutions.mediapipe.dev/pose).
"""


"""Upload any image that that has a person. We take two example images from the web: https://unsplash.com/photos/v4zceVZ5HK8 and https://unsplash.com/photos/e_rhazQLaSs.

"""


mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils 
mp_drawing_styles = mp.solutions.drawing_styles


class PoseCapture:


  def __init__(video=None):
    self.DESIRED_HEIGHT = 480
    self.DESIRED_WIDTH = 480
    if video == None:
      self.cap = cv2.VideoCapture(0)
    else:
      self.cap = c2.VideoCapture(video)
    self.images_path = imdir



  def resize_and_show(self, image):
    h, w = image.shape[:2]
    if h < w:
      img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
    else:
      img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
    cv2.imshow("",img)

  def angle(self, v1,v2,z_scale):
    v1[-1] /= z_scale
    v2[-1] /= z_scale
    dot = sum([v1[i] * v2[i] for i in range(len(v1))])
    norm1 = math.dist([0]*len(v1),v1)
    norm2 = math.dist([0]*len(v1),v2)
    #print(dot/(norm1*norm2))
    return math.acos(dot/(norm1*norm2))


  def landmarks_to_dict(self, landmarks):
    out = []
    # print("##############################")
    # print(landmarks.landmark)
    for (i,lm) in enumerate(landmarks.landmark):
      out.append(lm)
    return out

  def dict_to_list(self, d):
    return [d.x,d.y,d.z]

  def elbow_angle(self, results):
    landmarks = results.pose_world_landmarks
    lm_dict = landmarks_to_dict(landmarks)
    wrist = dict_to_list(lm_dict[PoseLandmark.RIGHT_WRIST])
    elbow = dict_to_list(lm_dict[PoseLandmark.RIGHT_ELBOW])
    shoulder = dict_to_list(lm_dict[PoseLandmark.RIGHT_SHOULDER])

    v1 = [shoulder[i] - elbow[i] for i in range(3)]
    v2 = [wrist[i] - elbow[i] for i in range(3)]
    return angle(v1,v2,30)

  def shoulder_angle(self, results):
    landmarks = results.pose_world_landmarks
    lm_dict = landmarks_to_dict(landmarks)
    hip = dict_to_list(lm_dict[PoseLandmark.RIGHT_HIP])
    elbow = dict_to_list(lm_dict[PoseLandmark.RIGHT_ELBOW])
    shoulder = dict_to_list(lm_dict[PoseLandmark.RIGHT_SHOULDER])

    v1 = [elbow[i] - shoulder[i] for i in range(3)]
    v2 = [hip[i] - shoulder[i] for i in range(3)]
    return angle(v1,v2,15)

  def shoulder_rotation(self, results):
    landmarks = results.pose_world_landmarks
    lm_dict = landmarks_to_dict(landmarks)
    elbow = dict_to_list(lm_dict[PoseLandmark.RIGHT_ELBOW])
    shoulder = dict_to_list(lm_dict[PoseLandmark.RIGHT_SHOULDER])

    v1 = [elbow[0] - shoulder[0], elbow[2] - shoulder[2]]
    v2 = [1,0]

    return angle(v1,v2,7)






# Read images with OpenCV.
#images = {"index": cv2.imread("index.jpeg")}
# Preview the images.
# for name, image in images.items():
#   print(name)   
#   resize_and_show(image)

"""All MediaPipe Solutions Python API examples are under mp.solutions.

For the MediaPipe Pose solution, we can access this module as `mp_pose = mp.solutions.pose`.

You may change the parameters, such as `static_image_mode` and `min_detection_confidence`, during the initialization. Run `help(mp_pose.Pose)` to get more informations about the parameters.
"""




  def angles():
    with mp_pose.Pose(
        min_detection_confidence=0.5,
        min_tracking_confidence=0.5,
        model_complexity=2) as pose:
      while self.cap.isOpened():
        success, image = cap.read()
        if not success:
          print("Ignoring empty camera frame.")
          # If loading a video, use 'break' instead of 'continue'.
          continue

        image.flags.writeable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = pose.process(image)
        yield [shoulder_rotation(results),shoulder_angle(results),elbow_angle(results),0,0,0]
        #print(elbow_angle(results))
        #print(shoulder_angle(results))
        # if(results.pose_world_landmarks and results.pose_world_landmarks.landmark):
        #   print("eblow: {}, shoulder: {}, roations: {} ".format(elbow_angle(results),shoulder_angle(results),shoulder_rotation(results)))
        #elbow_angle(results)
        #print(results.pose_landmarks)
        # print(mp_pose.POSE_CONNECTIONS)
        # Draw the pose annotation on the image.
        # image.flags.writeable = True
        #print(elbow_angle(results))
        # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        # mp_drawing.draw_landmarks(
        #     image,
        #     results.pose_landmarks,
        #     mp_pose.POSE_CONNECTIONS,
        #     landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
        # Flip the image horizontally for a selfie-view display.
        # cv2.imshow('MediaPipe Pose', cv2.flip(image, 1))
        # if cv2.waitKey(5) & 0xFF == 27:
        #   break
        # mp_drawing.plot_landmarks(
        #     results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)
    self.cap.release()


'''
# Run MediaPipe Pose and draw pose landmarks.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    # Convert the BGR image to RGB and process it with MediaPipe Pose.
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    print(elbow_angle(results))
    # Print nose landmark.
    image_hight, image_width, _ = image.shape
    if not results.pose_landmarks:
      continue
    print(
      f'Nose coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_hight})'
    )

    # Draw pose landmarks.
    print(f'Pose landmarks of {name}:')
    annotated_image = image.copy()
    mp_drawing.draw_landmarks(
        annotated_image,
        results.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
    resize_and_show(annotated_image)

# Run MediaPipe Pose and plot 3d pose world landmarks.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Print the real-world 3D coordinates of nose in meters with the origin at
    # the center between hips.
    print('Nose world landmark:'),
    print(results.pose_world_landmarks.landmark[mp_pose.PoseLandmark.NOSE])
    
    # Plot pose world landmarks.
    mp_drawing.plot_landmarks(
        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)

# Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, 
    model_complexity=2, enable_segmentation=True) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    # Draw pose segmentation.
    print(f'Pose segmentation of {name}:')
    annotated_image = image.copy()
    red_img = np.zeros_like(annotated_image, dtype=np.uint8)
    red_img[:, :] = (255,255,255)
    segm_2class = 0.2 + 0.8 * results.segmentation_mask
    segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)
    annotated_image = annotated_image * segm_2class + red_img * (1 - segm_2class)
    resize_and_show(annotated_image)
  '''
